---
title: "In the age of COVID, what predicts Zoom lecture attendance?"
author: "Ben Essex, Derek Che, & Chris Marston"
date: 04/22/2022
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(plotly)
library(GGally)
```

*******************************
**HARD DEADLINE: Wed 20 @ 6PM**
*******************************

## Introduction

#TODO (add outline comments)

**PROPOSED QUESTIONS** #TODO (we might want to slim these down a bit)

These are the two primary questions we originally set out to answer.

- "Do Covid case rates affect Zoom lecture attendance?"
- "Does weather predict Zoom lecture attendance?"

These are the original questions we had proposed broken down into a more "testable" format as per Scott's instructions.

- "Does weather predict Zoom lecture attendance?
    - "Does the temperature 2 hours before lecture affect Zoom lecture attendance?"
    - "Does the temperature 1 hour before lecture affect Zoom lecture attendance?"
    - "Does the temperature at the start of lecture affect Zoom lecture attendance?"
- "Do University of Utah Covid case rates affect Zoom lecture attendance?"
    - "Is Zoom lecture attendance affected by the total number of new Covid Cases at the University of Utah?"
    - "Is Zoom lecture attendance affected by the 7-day average of new Covid case rates at the University of Utah?"
- "Do Salt Lake County Covid case rates affect Zoom lecture attendance?"
    - "Is Zoom lecture attendance affected by the 7-day average case rate per 100,000 in SLC County?"
    - "Is Zoom lecture attendance affected by the SLC County Covid case rates per 100,000?"
    - "Is Zoom lecture attendance affected by the 7-day average positive test rate in SLC County?"

FOCUSED QUESTIONS LIST 

- "Does the mean temperature before class starts affect Zoom attendence?" (use the mean temp from T+2, T+1, & T=0)
- "Does the 7 day average case rate per 100k in Utah affect Zoom attendence?"
- "Does the Salt Lake County percent positive test rate affect Zoom attendence?"


## Data Collection

#TODO (add outline comments)

Due to the breadth of scope of the two questions being answered in this study, data was gathered from a range of sources.

**Salt Lake County Covid Data**

- Data was gathered from the Utah Covid Dashboard (overview) website [here](https://coronavirus-dashboard.utah.gov/overview.html) 

**University of Utah Covid Data**

- Data was gathered from the University of Utah Covid information page.
- Important Notes:
    - The U stopped posting Covid data to their public site partway through the collection period.
        - Currently attempting to get the data from them directly...

**Weather Data**

Weather data was pulled from:

- DarkSky for first part of data collection period.
- OpenWeather for last part of data collection period.
- Important Notes:
    - We switched data collection sources part way through the collection period.

**Zoom Attendance Data (headcounts)**

- Gathered directly by attending Zoom and recording attendee count by Zoom.
    - Professor excluded from count.
    - Excluded ourselves if we would have attended in person otherwise.
    - TAs were left in the counts - too difficult to remove them accurately.
    - Attendees counts were taken 10 min into the lecture, at the halfway point, and 10 min before the lecture ended.


## Methodology

#TODO (add outline comments)

## Analysis

#TODO (add outline comments)

```{r}
# import libraries
# data <- read csvs

# +-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
# |  Date |  T+10 | T+40 | T+70 | mean(...) | Temp T-2 | Temp T-1 | Temp T-0 | Weather | Class | UU New Cases | UU 7-Day Avg New Cases | SLCo Case Rate | SLCo 7-Day Avg Case Rate| SLCo 7-Day Avg Positive Test Rate | 
# +-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

# regression analyses:
#   - stimulus: nominal_weather, response: mean_attendance
#     - logistic regression(?)
#   - stimulus: temperature ->
#     - linear regression
#     - T-2 -> mean_attendance
#     - T-1 -> mean_attendance
#     - T-0 -> mean_attendance
#   - stimulus: COVID... ->
#     - linear regression
#     - uu_new_cases -> mean_attendance
#     - uu_new_cases_7day_avg -> mean_attendance
#     - slco_case_rate -> mean_attendance
#     - slco_case_rate_7day_avg -> mean_attendance
#     - slco_pos_test_rate_7day_avg -> mean_attendance
#   - more?

# correlation coefficients
#   - confidence interval
#   - point estimate
```

```{r}
# define dyplyr-able function to calcuate r^2
rsq <- function(.data, x, y) {
    rsq_(.data, lazyeval::lazy(x), lazyeval::lazy(y))
}

# define corrolary function_ since we're using non-standard evaluation (NSE)
rsq_ <- function(.data, x, y) {
    require(lazyeval)
    summary(lm(lazy_eval(x, .data)~lazy_eval(y, .data)))$r.squared
}

# calculate mean attendance and include NA
# calculate attendance rate based on number enrolled
# convert course into a factor
data <- read.csv("data/atd_weather_full.csv") %>%
    rowwise() %>%
    mutate(mean_atd = mean(c(atd_start, atd_mid, atd_end))) %>%
    mutate(atd_rate = mean_atd / enrolled) %>%
    mutate(course = factor(course)) %>%
    mutate(weather = factor(weather)) %>%
    mutate(date = as.Date(date, "%m/%d/%Y")) %>%
    mutate(mean_temp = mean(temp_tm2, temp_tm1, temp_tm0, na.rm = TRUE))

data %>%
    rsq(atd_rate, temp_tm2)

# let's focus on a handful of variables to start
data_filtered <- data %>%
    select(date, course, atd_rate, mean_temp,
           uu_new_cases, uu_new_cases_7da)

# this will be the general format for performing some
# exploratory analysis on variable pairs
p3500 <- data_filtered %>%
    filter(course == 3500) %>%
    select(!course) %>%
    ggpairs() +
    labs(title = "3500 Pairs")
```

```{r, include=FALSE}
p3130 <- data_filtered %>%
    filter(course == 3130) %>%
    select(!course) %>%
    ggpairs() +
    labs(title = "3130 Pairs")

p3200 <- data_filtered %>%
    filter(course == 3200) %>%
    select(!course) %>%
    ggpairs() +
    labs(title = "3200 Pairs")

p4400 <- data_filtered %>%
    filter(course == 4400) %>%
    select(!course) %>%
    ggpairs() +
    labs(title = "4400 Pairs")
    
p5140 <- data_filtered %>%
    filter(course == 5140) %>%
    select(!course) %>%
    ggpairs() +
    labs(title = "5140 Pairs")

```

```{r, warning=FALSE}
# now plot each of these:
ggplotly(p3500)
ggplotly(p3130)
ggplotly(p3200)
ggplotly(p4400)
ggplotly(p5140)
```

```{r}
data %>%
    ggplot(aes(temp_tm2, atd_rate, color = course)) +
    geom_point() +
    geom_smooth(method = "lm") +
    scale_color_manual(values = c("3130" = "purple",
                                  "3200" = "red",
                                  "5140" = "green",
                                  "3500" = "orange",
                                  "4400" = "blue"))

data %>%
    ggplot(aes(temp_tm1, atd_rate, color = course)) +
    geom_point() +
    geom_smooth(method = "lm") +
    scale_color_manual(values = c("3130" = "purple",
                                  "3200" = "red",
                                  "5140" = "green",
                                  "3500" = "orange",
                                  "4400" = "blue"))

data %>%
    ggplot(aes(temp_tm0, atd_rate, color = course)) +
    geom_point() +
    geom_smooth(method = "lm") +
    scale_color_manual(values = c("3130" = "purple",
                                  "3200" = "red",
                                  "5140" = "green",
                                  "3500" = "orange",
                                  "4400" = "blue"))

data_filtered %>%
    filter(course == "3130") %>%
    lattice::splom()

data_filtered %>%
    filter(course == "3500") %>%
    lattice::splom()

cor(data$temp_tm2, data$atd_rate, use = "complete.obs")
rsq(data, atd_rate, temp_tm2)

data %>%
    ggplot(aes(temp_tm1, atd_rate)) +
    geom_point() +
    geom_smooth(method = "lm")

data %>%
    filter(course == 3500) %>%
    ggplot(aes(temp_tm1, mean_atd)) +
    geom_point() +
    geom_smooth(method = "lm")


data %>%
    filter(course == 3500) %>%
    ggplot(aes(as.Date(date, "%m/%d/%Y"), mean_atd)) +
    geom_point() +
    geom_smooth(method = "lm")

# data_ben_3500 <- data_ben %>% 
#     filter(course == 3500) %>%
#     mutate(date = as.Date(date, "%m/%d/%Y")) %>%
#     rowwise() %>%
#     mutate(date_temp_tm2_ixn = as.numeric(date) * temp_tm2)

# cor(as.numeric(data_ben_3500$date), data_ben_3500$mean_atd, use = "complete.obs")
# rsq(data_ben_3500$date, data_ben_3500$mean_atd)
# rsq(data_ben_3500$date_temp_tm2_ixn, data_ben_3500$mean_atd)
# rsq(data_ben_3500$temp_tm2, data_ben_3500$mean_atd)

cor(data$temp_tm2, data$atd_rate, use = "complete.obs")
rsq(data, atd_rate, temp_tm2)

data %>%
    ggplot(aes(temp_tm0, atd_rate)) +
    geom_point() +
    geom_smooth(method = "lm")

cor(data$temp_tm2, data$atd_rate, use = "complete.obs")
rsq(data, atd_rate, temp_tm2)

data %>%
    ggplot(aes(date, atd_rate)) +
    geom_point() +
    geom_smooth(method = "lm")

atd_and_weather_data = read.csv("data/3130 Final Project Data Sheet - Atd & Weather.csv")
atd_daily_temp_and_covid = read.csv("data/3130 Final Project Data Sheet - Mean Atd Rate, Mean Daily Temp, & COVID-19.csv")

```



```{R}
# GENERATING VISUALIZATIONS




```

```{R}
# FUNCTIONS

# Confidence Interval Calculation: https://www.statology.org/confidence-interval-correlation-coefficient/
# z-score Calculation: https://www.statology.org/z-score-r/

Fisher_Transformation = function(correlation_coefficent) {
    return(
        log( ( (1 + correlation_coefficent) / (1 - correlation_coefficent) ) ) / 2
    )
}

# Use our corellation coefficent for the data_value here
Z_Score = function(data_value, sample_mean, sample_sd) {
    return(
        ( data_value - sample_mean ) / sample_sd
    )
}

Log_Bounds = function(upper, fisher_transformed, z_score, sample_size) {
    if (upper == TRUE) # Calculate the upper bound
    {
        return(
            fisher_transformed + ( z_score / sqrt( sample_size - 3) )
        )
    }
    else # If upper is false, default to calculating the lower bound
    {
        return(
            fisher_transformed - ( z_score / sqrt( sample_size - 3) )
        )
    }
}

CI_Upper = function(log_upper_bound) {
    return(
        (exp(2 * log_upper_bound) - 1) / (exp(2 * log_upper_bound) + 1 )
    )
}

CI_Lower = function(log_lower_bound) {
    return(
        (exp(2 * log_lower_bound) - 1) / (exp(2 * log_lower_bound) + 1 )
    )
}


```

```{r}
# CORRELATION COEFFICIENT & CONFIDENCE INTERVALS FOR mean_daily_atd_rate --- mean_daily_temp

# Find the Correlation Coefficent
mean_daily_temp_correlation = cor(atd_daily_temp_and_covid$mean_daily_atd_rate,
                                  atd_daily_temp_and_covid$mean_daily_temp)

# Perform Fisher Transformation (this make the data "normally distributed" so that we can properly generate a confidence interval, which requires normally distributed data)
fisher_transformed = Fisher_Transformation( mean_daily_temp_correlation )

# Calculate z-score
z_score = 1.96 # 95% confidence - assumes normally distributed data

# Find upper and lower bounds
log_upper_bound = Log_Bounds(TRUE, fisher_transformed, z_score, nrow( atd_daily_temp_and_covid ) )

log_lower_bound = Log_Bounds(FALSE, fisher_transformed, z_score, nrow( atd_daily_temp_and_covid ) )

# Find the confidence interval
CI_upper_atd_vs_temp = CI_Upper(log_upper_bound)

CI_lower_atd_vs_temp = CI_Lower(log_lower_bound)

```

```{r}
# CORRELATION COEFFICIENT & CONFIDENCE INFERVALS FOR mean_daily_atd_rate --- daily_confirmed_case_count

# Find the Correlation Coefficent
mean_daily_case_count_correlation = cor(atd_daily_temp_and_covid$mean_daily_atd_rate,
                                        atd_daily_temp_and_covid$daily_confirmed_case_count)

# Perform Fisher Transformation (this make the data "normally distributed" so that we can properly generate a confidence interval, which requires normally distributed data)
fisher_transformed = Fisher_Transformation( mean_daily_case_count_correlation )

# Calculate z-score
z_score = 1.96 # 95% confidence

# Find upper and lower bounds
log_upper_bound = Log_Bounds(TRUE, fisher_transformed, z_score, nrow( atd_daily_temp_and_covid ) )

log_lower_bound = Log_Bounds(FALSE, fisher_transformed, z_score, nrow( atd_daily_temp_and_covid ) )

# Find the confidence interval
CI_upper_atd_vs_case_count = CI_Upper(log_upper_bound)

CI_lower_atd_vs_case_count = CI_Lower(log_lower_bound)

```

```{r}
# CORRELATION COEFFICIENT & CONFIDENCE INTERVALS FOR mean_daily_atd_rate --- confirmed_case_count_7d_avg (confirmed_7dcc)

# Find the Correlation Coefficent
mean_daily_confirmed_7dcc_correlation = cor(atd_daily_temp_and_covid$mean_daily_atd_rate,
                                  atd_daily_temp_and_covid$confirmed_case_count_7d_avg)

# Perform Fisher Transformation (this make the data "normally distributed" so that we can properly generate a confidence interval, which requires normally distributed data)
fisher_transformed = Fisher_Transformation( mean_daily_confirmed_7dcc_correlation )

# Calculate z-score
z_score = 1.96 # 95% confidence

# Find upper and lower bounds
log_upper_bound = Log_Bounds(TRUE, fisher_transformed, z_score, nrow( atd_daily_temp_and_covid ) )

log_lower_bound = Log_Bounds(FALSE, fisher_transformed, z_score, nrow( atd_daily_temp_and_covid ) )

# Find the confidence interval
CI_upper_atd_vs_confirmed_7dcc = CI_Upper(log_upper_bound)

CI_lower_atd_vs_confirmed_7dcc = CI_Lower(log_lower_bound)

```

```{r}
# CORRELATION COEFFICIENT & CONFIDENCE INTERVALS FOR mean_daily_atd_rate --- positive_test_rate_7d_avg (positive_7dtr)

# Find the Correlation Coefficent
mean_daily_positive_7dtr_correlation = cor(atd_daily_temp_and_covid$mean_daily_atd_rate,
                                  atd_daily_temp_and_covid$positive_test_rate_7d_avg)

# Perform Fisher Transformation (this make the data "normally distributed" so that we can properly generate a confidence interval, which requires normally distributed data)
fisher_transformed = Fisher_Transformation( mean_daily_positive_7dtr_correlation )

# Calculate z-score
z_score = 1.96 # 95% confidence

# Find upper and lower bounds
log_upper_bound = Log_Bounds(TRUE, fisher_transformed, z_score, nrow( atd_daily_temp_and_covid ) )

log_lower_bound = Log_Bounds(FALSE, fisher_transformed, z_score, nrow( atd_daily_temp_and_covid ) )

# Find the confidence interval
CI_upper_atd_vs_positive_7dtr = CI_Upper(log_upper_bound)

CI_lower_atd_vs_positive_7dtr = CI_Lower(log_lower_bound)

```




## Limitations

#TODO (add outline comments)

## Conclusions

#TODO (add outline comments)