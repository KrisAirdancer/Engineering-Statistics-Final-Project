---
title: "In the age of COVID, what predicts Zoom lecture attendance?"
author: "Ben Essex, Derek Che, & Chris Marston"
date: 04/22/2022
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(plotly)
library(GGally)
```

*******************************
**HARD DEADLINE: Wed 20 @ 6PM**
*******************************

## Introduction

#TODO (add outline comments)

**PROPOSED QUESTIONS** #TODO (we might want to slim these down a bit)

These are the two primary questions we originally set out to answer.

- "Do Covid case rates affect Zoom lecture attendance?"
- "Does weather predict Zoom lecture attendance?"

These are the original questions we had proposed broken down into a more "testable" format as per Scott's instructions.

- "Does weather predict Zoom lecture attendance?
    - "Does the temperature 2 hours before lecture affect Zoom lecture attendance?"
    - "Does the temperature 1 hour before lecture affect Zoom lecture attendance?"
    - "Does the temperature at the start of lecture affect Zoom lecture attendance?"
- "Do University of Utah Covid case rates affect Zoom lecture attendance?"
    - "Is Zoom lecture attendance affected by the total number of new Covid Cases at the University of Utah?"
    - "Is Zoom lecture attendance affected by the 7-day average of new Covid case rates at the University of Utah?"
- "Do Salt Lake County Covid case rates affect Zoom lecture attendance?"
    - "Is Zoom lecture attendance affected by the 7-day average case rate per 100,000 in SLC County?"
    - "Is Zoom lecture attendance affected by the SLC County Covid case rates per 100,000?"
    - "Is Zoom lecture attendance affected by the 7-day average positive test rate in SLC County?"

FOCUSED QUESTIONS LIST 

- "Does the mean temperature before class starts affect Zoom attendence?" (use the mean temp from T+2, T+1, & T=0)
- "Does the 7 day average case rate per 100k in Utah affect Zoom attendence?"
- "Does the Salt Lake County percent positive test rate affect Zoom attendence?"


## Data Collection

#TODO (add outline comments)

Due to the breadth of scope of the two questions being answered in this study, data was gathered from a range of sources.

**Salt Lake County Covid Data**

- Data was gathered from the Utah Covid Dashboard (overview) website [here](https://coronavirus-dashboard.utah.gov/overview.html) 

**University of Utah Covid Data**

- Data was gathered from the University of Utah Covid information page.
- Important Notes:
    - The U stopped posting Covid data to their public site partway through the collection period.
        - Currently attempting to get the data from them directly...

**Weather Data**

Weather data was pulled from:

- DarkSky for first part of data collection period.
- OpenWeather for last part of data collection period.
- Important Notes:
    - We switched data collection sources part way through the collection period.

**Zoom Attendance Data (headcounts)**

- Gathered directly by attending Zoom and recording attendee count by Zoom.
    - Professor excluded from count.
    - Excluded ourselves if we would have attended in person otherwise.
    - TAs were left in the counts - too difficult to remove them accurately.
    - Attendees counts were taken 10 min into the lecture, at the halfway point, and 10 min before the lecture ended.


## Methodology

#TODO (add outline comments)

## Analysis

#TODO (add outline comments)

In performing a more in-depth analysis with Pearson correlations, we must first define a function to calculate the Pearson coefficient of determination, or $r^2$. This number, which will fall between 0 and 1, shows what percentage of the variation in the response (dependent) variable is explained by the explanatory (independent) variable.

Since we are using `dplyr` for much of this analysis, we can define the function using `lazyeval` to enable the non-standard evaluation used by `dplyr`.

```{r}
# define dyplyr-able function to calcuate r^2
rsq <- function(.data, x, y) {
    rsq_(.data, lazyeval::lazy(x), lazyeval::lazy(y))
}

# define corrolary function_ since we're using non-standard evaluation (NSE)
rsq_ <- function(.data, x, y) {
    require(lazyeval)
    summary(lm(lazy_eval(x, .data)~lazy_eval(y, .data)))$r.squared
}

# bonus:
# define dyplyr-able function to calculate Pearson correlation coefficient
cor <- function(.data, x, y) {
    cor_(.data, lazyeval::lazy(x), lazyeval::lazy(y))
}

# define corrolary function_ since we're using non-standard evaluation (NSE)
cor_ <- function(.data, x, y) {
    stats::cor(x = lazyeval::lazy_eval(x, .data),
               y = lazyeval::lazy_eval(y, .data),
               use = "complete.obs")
}
```

With this function defined, we ingest the raw data and appropriately transform, or "wrangle" it, such that we are able to vizualize and analyze it.

```{r}
# compute mean attendance and propagate NA
# compute attendance rate based on number enrolled
# convert course into a factor
# convert weather into a factor
# make date R-readable
# compute mean of temps 2 hours before class
data <- read.csv("data/atd_weather_full.csv") %>%
    rowwise() %>%
    mutate(mean_atd = mean(c(atd_start, atd_mid, atd_end))) %>%
    mutate(atd_rate = mean_atd / enrolled) %>%
    mutate(course = factor(course)) %>%
    mutate(weather = factor(weather)) %>%
    mutate(date = as.Date(date, "%m/%d/%Y")) %>%
    mutate(mean_temp = mean(temp_tm2, temp_tm1, temp_tm0, na.rm = TRUE))
```

Now that the data has been ingested, we pare down our explanatory variables to only those at the interval level of measurement. This includes: `mean_temp`, which represents the mean of temperature 2 hours before, 1 hour before, and at the time class begins; `uu_new_cases`, which represents the number of daily new COVID cases were recorded at the University of Utah; and `uu_new_cases_7da`, which represents a rolling, 7-day average of daily new COVID cases recorded at the University of Utah. We also include `atd_rate`, our response variable, which we compute by taking the mean of Zoom lecture attendees at each of our three data points, then dividing this number by the number of students enrolled in the course at the end of the data collection period.

Our goal here is to begin by focusing on only a handful of variables, which we hope will allow us to quickly see trends and correlations across many pairs of variables.

```{r}
# filtering down to a choice few variables
data_filtered <- data %>%
    select(date, course, atd_rate, mean_temp,
           uu_new_cases, uu_new_cases_7da)
```

With our data fully transformed, we can begin to plot correlations. The quickest way bootstrap this process is by making a scatter plot with `ggplot2` and visually inspecting the initial results.

```{r}
# begin by looking at mean temp
data_filtered %>%
    ggplot(aes(mean_temp, atd_rate, color = course)) +
    geom_point() +
    geom_smooth(method = "lm") +
    scale_color_manual(values = c("3130" = "purple",
                                  "3200" = "red",
                                  "5140" = "green",
                                  "3500" = "orange",
                                  "4400" = "blue"))
```

When we begin by looking at the correlation between `mean_temp` and `atd_rate`, shown above, we immediately see class-wise trends emerge. Even with bands of standard error, we see a fairly good fit for 4400, which trends slightly upward, and for 3500, which trends sideways. 3200, despite having wider error bounds, seems to mostly trend downward. Since we see different trends with different classes, it suggests class could be confounding variable we hadn't earlier considered. This reality highlights the possibility that there may be unmeasured characteristics of the class itself that may interact with `mean_temp` and a student's likelihood to attend class via Zoom. For example, it's possible that for difficult or very important classes, students are relatively "inelastic", or un-sensitive, to changes in weather, preferring to always attend in person. While we should nod to this possibility, we lack the data and instrumentation to fully capture this variable and decompose it into its atomic parts, which is a limitation of this study.

We can continue with `ggplot2`, now looking at `uu_new_cases` and `uu_new_cases_7da` as explanatory variables.

```{r,warning=FALSE}
# Explanatory: `uu_new_cases`
data_filtered %>%
    ggplot(aes(uu_new_cases, atd_rate, color = course)) +
    geom_point() +
    geom_smooth(method = "lm") +
    scale_color_manual(values = c("3130" = "purple",
                                  "3200" = "red",
                                  "5140" = "green",
                                  "3500" = "orange",
                                  "4400" = "blue"))

# Explanatory: `uu_new_cases_7da`
data_filtered %>%
    ggplot(aes(uu_new_cases_7da, atd_rate, color = course)) +
    geom_point() +
    geom_smooth(method = "lm") +
    scale_color_manual(values = c("3130" = "purple",
                                  "3200" = "red",
                                  "5140" = "green",
                                  "3500" = "orange",
                                  "4400" = "blue"))
```

For `uu_new_cases`, the first thing we notice is that there are very few data points at the very high end of the x-axis. This is because, toward the beginning of our data collection period, there were 1 or 2 days of collection that occurred right at the end of a COVID surge. For 3200 and 4400, we see that the trend-lines do not even extend beyond around 50 cases. Since data collection for these classes began just a day or two after the surge had already dropped back down, no observations exist here at the higher end of our explanatory variable for these classes. Furthermore, with so few data at this high end, the error margins are too large for us to draw any preliminary conclusions from these trends. We might look at 5140, for which Zoom attendance rate appears to increase perhaps linearly or logarithmically with daily new cases. But then we could also look at 3500, for example, for which Zoom attendance rate almost certainly decreases with the daily case count.

If we look at the 7-day average, `uu_new_cases_7da`, we get slightly more interesting results. Again, margins of error here make our results from 5140 and 3130 not immediately useful to us. However, we can see trends emerging again from 3200, 4400, and 3500, for which Zoom attendance rate appears to increase, move sideways, and decrease with average cases, respectively.

With some initial trends in mind, we can begin computing correlation pairs for each class. This is a way to get slightly better depth of information, though we're still in the exploratory stage of looking at the gathered data.

```{r}
# This will be the general format for performing some
# exploratory analysis on variable pairs.
# We do this for each class.
p3500 <- data_filtered %>%
    filter(course == 3500) %>%
    select(!course) %>%
    ggpairs() +
    labs(title = "3500 Pairs")
```

```{r, include=FALSE}
p3130 <- data_filtered %>%
    filter(course == 3130) %>%
    select(!course) %>%
    ggpairs() +
    labs(title = "3130 Pairs")

p3200 <- data_filtered %>%
    filter(course == 3200) %>%
    select(!course) %>%
    ggpairs() +
    labs(title = "3200 Pairs")

p4400 <- data_filtered %>%
    filter(course == 4400) %>%
    select(!course) %>%
    ggpairs() +
    labs(title = "4400 Pairs")
    
p5140 <- data_filtered %>%
    filter(course == 5140) %>%
    select(!course) %>%
    ggpairs() +
    labs(title = "5140 Pairs")

```

Now that we have prepared our plots, we can plot them one-by-one.

```{r, warning=FALSE}
ggplotly(p3500)
```

With 3500, we see a few interesting correlations. We see `uu_new_cases_7da` has a significant negative correlation with `atd_rate` ($p < 0.001$), `uu_new_cases` with `atd_rate` ($p < 0.01$). Mean temperature seems to have no significance here.

```{r, warning=FALSE}
ggplotly(p3130)
```

With 3130, we see no interesting correlations of any significance. We see a correlation between `uu_new_cases_7da` and `uu_new_cases`, which we would expect for trivial reasons. We also see a correlation between case rates and date, which supports an assumption we alluded to earlier--that case rates have mostly decreased as the data collection period went on.

```{r, warning=FALSE}
ggplotly(p3200)
```

With 3200, we see some correlations of significance. `uu_new_cases_7da` and `uu_new_cases` appear to have strong positive correlations with `atd_rate` ($p < 0.01$), and `mean_temp` a slightly weaker, negative correlation with `atd_rate` ($p < 0.01$). We also see a correlation between `atd_rate` and `date` ($p < 0.001$), which, since case rates also correlate with date, makes it difficult to infer causality in the earlier correlations. What looks like COVID case rates predicting attendance might in fact be dropping Zoom attendance as the semester goes on, unrelated to COVID case numbers.

```{r, warning=FALSE}
ggplotly(p4400)
```

For 4400, we see a fairly strong positive correlation between `mean_temp` and `atd_rate` ($p < 0.01$) and between `date` and `atd_rate` ($p < 0.01$).

```{r, warning=FALSE}
ggplotly(p5140)
```

Finally, for 5140, we see essentially no correlations of interest and significance.


```{r}
data_3500 <- data_filtered %>%
    filter(course == 3500)

cor(data_3500$uu_new_cases_7da, data_3500$atd_rate, use = "complete.obs")


cor(data_filtered$uu_new_cases_7da, data_filtered$atd_rate, use = "complete.obs")
# data %>%
#     ggplot(aes(temp_tm1, atd_rate, color = course)) +
#     geom_point() +
#     geom_smooth(method = "lm") +
#     scale_color_manual(values = c("3130" = "purple",
#                                   "3200" = "red",
#                                   "5140" = "green",
#                                   "3500" = "orange",
#                                   "4400" = "blue"))

# data %>%
#     ggplot(aes(temp_tm0, atd_rate, color = course)) +
#     geom_point() +
#     geom_smooth(method = "lm") +
#     scale_color_manual(values = c("3130" = "purple",
#                                   "3200" = "red",
#                                   "5140" = "green",
#                                   "3500" = "orange",
#                                   "4400" = "blue"))

# data_filtered %>%
#     filter(course == "3130") %>%
#     lattice::splom()

# data_filtered %>%
#     filter(course == "3500") %>%
#     lattice::splom()

cor(data$temp_tm2, data$atd_rate, use = "complete.obs")
rsq(data, atd_rate, temp_tm2)

data %>%
    ggplot(aes(temp_tm1, atd_rate)) +
    geom_point() +
    geom_smooth(method = "lm")

data %>%
    filter(course == 3500) %>%
    ggplot(aes(temp_tm1, mean_atd)) +
    geom_point() +
    geom_smooth(method = "lm")


data %>%
    filter(course == 3500) %>%
    ggplot(aes(as.Date(date, "%m/%d/%Y"), mean_atd)) +
    geom_point() +
    geom_smooth(method = "lm")

cor(data$temp_tm2, data$atd_rate, use = "complete.obs")
rsq(data, atd_rate, temp_tm2)

data %>%
    ggplot(aes(temp_tm0, atd_rate)) +
    geom_point() +
    geom_smooth(method = "lm")

cor(data$temp_tm2, data$atd_rate, use = "complete.obs")
rsq(data, atd_rate, temp_tm2)

data %>%
    ggplot(aes(date, atd_rate)) +
    geom_point() +
    geom_smooth(method = "lm")
```


```{r}
atd_and_weather_data = read.csv("data/3130 Final Project Data Sheet - Atd & Weather.csv")
atd_daily_temp_and_covid = read.csv("data/3130 Final Project Data Sheet - Mean Atd Rate, Mean Daily Temp, & COVID-19.csv")

```



```{R}
# GENERATING VISUALIZATIONS




```

```{R}
# FUNCTIONS

# Confidence Interval Calculation: https://www.statology.org/confidence-interval-correlation-coefficient/
# z-score Calculation: https://www.statology.org/z-score-r/

Fisher_Transformation = function(correlation_coefficent) {
    return(
        log( ( (1 + correlation_coefficent) / (1 - correlation_coefficent) ) ) / 2
    )
}

# Use our corellation coefficent for the data_value here
Z_Score = function(data_value, sample_mean, sample_sd) {
    return(
        ( data_value - sample_mean ) / sample_sd
    )
}

Log_Bounds = function(upper, fisher_transformed, z_score, sample_size) {
    if (upper == TRUE) # Calculate the upper bound
    {
        return(
            fisher_transformed + ( z_score / sqrt( sample_size - 3) )
        )
    }
    else # If upper is false, default to calculating the lower bound
    {
        return(
            fisher_transformed - ( z_score / sqrt( sample_size - 3) )
        )
    }
}

CI_Upper = function(log_upper_bound) {
    return(
        (exp(2 * log_upper_bound) - 1) / (exp(2 * log_upper_bound) + 1 )
    )
}

CI_Lower = function(log_lower_bound) {
    return(
        (exp(2 * log_lower_bound) - 1) / (exp(2 * log_lower_bound) + 1 )
    )
}


```

```{r}
# CORRELATION COEFFICIENT & CONFIDENCE INTERVALS FOR mean_daily_atd_rate --- mean_daily_temp

# Find the Correlation Coefficent
mean_daily_temp_correlation = cor(atd_daily_temp_and_covid$mean_daily_atd_rate,
                                  atd_daily_temp_and_covid$mean_daily_temp)

# Perform Fisher Transformation (this make the data "normally distributed" so that we can properly generate a confidence interval, which requires normally distributed data)
fisher_transformed = Fisher_Transformation( mean_daily_temp_correlation )

# Calculate z-score
z_score = 1.96 # 95% confidence - assumes normally distributed data

# Find upper and lower bounds
log_upper_bound = Log_Bounds(TRUE, fisher_transformed, z_score, nrow( atd_daily_temp_and_covid ) )

log_lower_bound = Log_Bounds(FALSE, fisher_transformed, z_score, nrow( atd_daily_temp_and_covid ) )

# Find the confidence interval
CI_upper_atd_vs_temp = CI_Upper(log_upper_bound)

CI_lower_atd_vs_temp = CI_Lower(log_lower_bound)

```

```{r}
# CORRELATION COEFFICIENT & CONFIDENCE INFERVALS FOR mean_daily_atd_rate --- daily_confirmed_case_count

# Find the Correlation Coefficent
mean_daily_case_count_correlation = cor(atd_daily_temp_and_covid$mean_daily_atd_rate,
                                        atd_daily_temp_and_covid$daily_confirmed_case_count)

# Perform Fisher Transformation (this make the data "normally distributed" so that we can properly generate a confidence interval, which requires normally distributed data)
fisher_transformed = Fisher_Transformation( mean_daily_case_count_correlation )

# Calculate z-score
z_score = 1.96 # 95% confidence

# Find upper and lower bounds
log_upper_bound = Log_Bounds(TRUE, fisher_transformed, z_score, nrow( atd_daily_temp_and_covid ) )

log_lower_bound = Log_Bounds(FALSE, fisher_transformed, z_score, nrow( atd_daily_temp_and_covid ) )

# Find the confidence interval
CI_upper_atd_vs_case_count = CI_Upper(log_upper_bound)

CI_lower_atd_vs_case_count = CI_Lower(log_lower_bound)

```

```{r}
# CORRELATION COEFFICIENT & CONFIDENCE INTERVALS FOR mean_daily_atd_rate --- confirmed_case_count_7d_avg (confirmed_7dcc)

# Find the Correlation Coefficent
mean_daily_confirmed_7dcc_correlation = cor(atd_daily_temp_and_covid$mean_daily_atd_rate,
                                  atd_daily_temp_and_covid$confirmed_case_count_7d_avg)

# Perform Fisher Transformation (this make the data "normally distributed" so that we can properly generate a confidence interval, which requires normally distributed data)
fisher_transformed = Fisher_Transformation( mean_daily_confirmed_7dcc_correlation )

# Calculate z-score
z_score = 1.96 # 95% confidence

# Find upper and lower bounds
log_upper_bound = Log_Bounds(TRUE, fisher_transformed, z_score, nrow( atd_daily_temp_and_covid ) )

log_lower_bound = Log_Bounds(FALSE, fisher_transformed, z_score, nrow( atd_daily_temp_and_covid ) )

# Find the confidence interval
CI_upper_atd_vs_confirmed_7dcc = CI_Upper(log_upper_bound)

CI_lower_atd_vs_confirmed_7dcc = CI_Lower(log_lower_bound)

```

```{r}
# CORRELATION COEFFICIENT & CONFIDENCE INTERVALS FOR mean_daily_atd_rate --- positive_test_rate_7d_avg (positive_7dtr)

# Find the Correlation Coefficent
mean_daily_positive_7dtr_correlation = cor(atd_daily_temp_and_covid$mean_daily_atd_rate,
                                  atd_daily_temp_and_covid$positive_test_rate_7d_avg)

# Perform Fisher Transformation (this make the data "normally distributed" so that we can properly generate a confidence interval, which requires normally distributed data)
fisher_transformed = Fisher_Transformation( mean_daily_positive_7dtr_correlation )

# Calculate z-score
z_score = 1.96 # 95% confidence

# Find upper and lower bounds
log_upper_bound = Log_Bounds(TRUE, fisher_transformed, z_score, nrow( atd_daily_temp_and_covid ) )

log_lower_bound = Log_Bounds(FALSE, fisher_transformed, z_score, nrow( atd_daily_temp_and_covid ) )

# Find the confidence interval
CI_upper_atd_vs_positive_7dtr = CI_Upper(log_upper_bound)

CI_lower_atd_vs_positive_7dtr = CI_Lower(log_lower_bound)

```




## Limitations

#TODO (add outline comments)

## Conclusions

#TODO (add outline comments)